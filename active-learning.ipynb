{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de55619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import graphlearning as gl\n",
    "\n",
    "mnist_digits, mnist_labels = gl.datasets.load(\"mnist\")\n",
    "\n",
    "mnist_X = torch.tensor(mnist_digits, dtype=torch.float32).reshape(-1, 1, 28, 28) / 255.0\n",
    "mnist_y = torch.tensor(mnist_labels, dtype=torch.long)\n",
    "\n",
    "cifar, cifar_labels = gl.datasets.load(\"cifar10\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c0683",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "Datasets:\n",
    "1. CIFAR-10\n",
    "2. MNIST\n",
    "\n",
    "Models:\n",
    "1. Convolutional Features\n",
    "2. ReLU Features\n",
    "3. Fourier Features\n",
    "\n",
    "Each model transforms the data to a feature matrix $[M_{TM} | M_{TU}]$ where $M_{TM}$ is the data matrix for the training set and $M_{TU}$ are the basis functions that we have not yet modeled. We will compute the best coefficients, $\\tilde{c}$ of basis functions to model the labels on the modeled training set and the best coefficients, $c$, of all basis functions to model the labels on the whole training set. We will then compute the error $c_{err} = \\tilde{c}-c^*$ where $c^*$ is the truncated version of $c$ to match the size of $\\tilde{c}$. We initialize $c$ with the least-squares coefficients learned from the whole training set. Then we compute $\\tilde{c}$ by solving the least-squares problem on the sampled training set. \n",
    "\n",
    "For each dataset, we will:\n",
    "- Sample the features uniformly at random vs by leverage scores.\n",
    "- Plot $||A||_2$, $||M_{TM}^+||_2$, and $||\\tilde{c}-c^*||_2$ for the sampled features as a function of the number of sampled points.\n",
    "\n",
    "We expect to see that leverage score sampling leads to a smaller error $||\\tilde{c}-c^*||_2$ for the same number of sampled points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29ab21d",
   "metadata": {},
   "source": [
    "\n",
    "## MNIST\n",
    "\n",
    "The MNIST dataset consists of 70,000 images of handwritten digits (0-9) in grayscale with a resolution of 28x28 pixels. This gives us a $70,000 \\times 784$ data matrix.\n",
    "- A Convolutional Neural network will transform the data to a $70,000 \\times 200$ matrix (by removing the last layer).\n",
    "- A Random ReLU fully-connected network ($y({\\textbf{t}}) = \\sum_{k=1}^{200} w_k \\sigma(\\left<\\textbf{t}, {\\textbf{v}}_k\\right>)$ with $\\sigma(x) = \\max(0,x)$ and $\\textbf{v}_k$ being randomly initialized weights and $w_k$ being the learned coefficients) will transform the data to a $70,000 \\times 200$ matrix.\n",
    "- A Fourier fully-connected network ($y({\\textbf{t}}) = \\sum_{k=1}^{200} w_k \\exp(i\\pi\\left<\\textbf{t}, {\\textbf{v}}_k\\right>)$ with $\\textbf{v}_k$ being randomly initialized weights and $w_k$ being the learned coefficients) will transform the data to a $70,000 \\times 200$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d7db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9213\n"
     ]
    }
   ],
   "source": [
    "# Verify CNN accuracy on MNIST\n",
    "\n",
    "from models.mnist_cnn import ConvNet, BASIS_FUNCTIONS\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "network = ConvNet()\n",
    "network.load_state_dict(torch.load(\"models/mnist_cnn.pth\", map_location=device))\n",
    "network.eval()\n",
    "\n",
    "def verify_mnist_cnn(model: ConvNet, device):\n",
    "    model.to(device)\n",
    "    indices = torch.randperm(mnist_X.shape[0])\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        TensorDataset(mnist_X[indices], mnist_y[indices]),\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds: torch.Tensor = model(xb)\n",
    "            correct += (preds.argmax(1) == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    print(f\"Test accuracy: {correct / total:.4f}\")\n",
    "verify_mnist_cnn(network, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea998e6",
   "metadata": {},
   "source": [
    "We get an accuracy on the whole dataset of `0.9213`. Pretty good. Now we can embed the data using the convolutional layers of the network and use that as our feature matrix for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "874f8441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 200)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embed the data using the convolutional layers of the network\n",
    "mnist_cnn_embedding = np.empty((mnist_X.shape[0], BASIS_FUNCTIONS))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_start in range(0, mnist_X.shape[0], 256):\n",
    "        batch_end = min(batch_start + 256, mnist_X.shape[0])\n",
    "        batch = mnist_X[batch_start:batch_end].to(device)\n",
    "        embeddings = network.embed(batch).cpu().numpy()\n",
    "        mnist_cnn_embedding[batch_start:batch_end] = embeddings\n",
    "\n",
    "mnist_cnn_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d91274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 10 points...\n",
      "Sampling 110 points...\n",
      "Sampling 210 points...\n",
      "Sampling 310 points...\n",
      "Sampling 410 points...\n",
      "Sampling 510 points...\n",
      "Sampling 610 points...\n",
      "Sampling 710 points...\n",
      "Sampling 810 points...\n",
      "Sampling 910 points...\n",
      "Sampling 1010 points...\n",
      "Sampling 1110 points...\n",
      "Sampling 1210 points...\n",
      "Sampling 1310 points...\n",
      "Sampling 1410 points...\n",
      "Sampling 1510 points...\n",
      "Sampling 1610 points...\n",
      "Sampling 1710 points...\n",
      "Sampling 1810 points...\n",
      "Sampling 1910 points...\n",
      "Sampling 2010 points...\n",
      "Sampling 2110 points...\n",
      "Sampling 2210 points...\n",
      "Sampling 2310 points...\n",
      "Sampling 2410 points...\n",
      "Sampling 2510 points...\n",
      "Sampling 2610 points...\n",
      "Sampling 2710 points...\n",
      "Sampling 2810 points...\n",
      "Sampling 2910 points...\n",
      "Sampling 3010 points...\n",
      "Sampling 3110 points...\n",
      "Sampling 3210 points...\n",
      "Sampling 3310 points...\n",
      "Sampling 3410 points...\n",
      "Sampling 3510 points...\n",
      "Sampling 3610 points...\n",
      "Sampling 3710 points...\n",
      "Sampling 3810 points...\n",
      "Sampling 3910 points...\n",
      "Sampling 4010 points...\n",
      "Sampling 4110 points...\n",
      "Sampling 4210 points...\n",
      "Sampling 4310 points...\n",
      "Sampling 4410 points...\n",
      "Sampling 4510 points...\n",
      "Sampling 4610 points...\n",
      "Sampling 4710 points...\n",
      "Sampling 4810 points...\n",
      "Sampling 4910 points...\n",
      "Sampling 5010 points...\n",
      "Sampling 5110 points...\n",
      "Sampling 5210 points...\n",
      "Sampling 5310 points...\n",
      "Sampling 5410 points...\n",
      "Sampling 5510 points...\n",
      "Sampling 5610 points...\n",
      "Sampling 5710 points...\n",
      "Sampling 5810 points...\n",
      "Sampling 5910 points...\n",
      "Sampling 6010 points...\n",
      "Sampling 6110 points...\n",
      "Sampling 6210 points...\n",
      "Sampling 6310 points...\n",
      "Sampling 6410 points...\n",
      "Sampling 6510 points...\n",
      "Sampling 6610 points...\n",
      "Sampling 6710 points...\n",
      "Sampling 6810 points...\n",
      "Sampling 6910 points...\n",
      "Sampling 7010 points...\n",
      "Sampling 7110 points...\n",
      "Sampling 7210 points...\n",
      "Sampling 7310 points...\n",
      "Sampling 7410 points...\n",
      "Sampling 7510 points...\n",
      "Sampling 7610 points...\n",
      "Sampling 7710 points...\n",
      "Sampling 7810 points...\n",
      "Sampling 7910 points...\n",
      "Sampling 8010 points...\n",
      "Sampling 8110 points...\n",
      "Sampling 8210 points...\n",
      "Sampling 8310 points...\n",
      "Sampling 8410 points...\n",
      "Sampling 8510 points...\n",
      "Sampling 8610 points...\n",
      "Sampling 8710 points...\n",
      "Sampling 8810 points...\n",
      "Sampling 8910 points...\n",
      "Sampling 9010 points...\n",
      "Sampling 9110 points...\n",
      "Sampling 9210 points...\n",
      "Sampling 9310 points...\n",
      "Sampling 9410 points...\n",
      "Sampling 9510 points...\n",
      "Sampling 9610 points...\n",
      "Sampling 9710 points...\n",
      "Sampling 9810 points...\n",
      "Sampling 9910 points...\n",
      "Sampling 10010 points...\n",
      "Sampling 10110 points...\n",
      "Sampling 10210 points...\n",
      "Sampling 10310 points...\n",
      "Sampling 10410 points...\n",
      "Sampling 10510 points...\n",
      "Sampling 10610 points...\n",
      "Sampling 10710 points...\n",
      "Sampling 10810 points...\n",
      "Sampling 10910 points...\n",
      "Sampling 11010 points...\n",
      "Sampling 11110 points...\n",
      "Sampling 11210 points...\n",
      "Sampling 11310 points...\n",
      "Sampling 11410 points...\n",
      "Sampling 11510 points...\n",
      "Sampling 11610 points...\n",
      "Sampling 11710 points...\n",
      "Sampling 11810 points...\n",
      "Sampling 11910 points...\n",
      "Sampling 12010 points...\n",
      "Sampling 12110 points...\n",
      "Sampling 12210 points...\n",
      "Sampling 12310 points...\n",
      "Sampling 12410 points...\n",
      "Sampling 12510 points...\n",
      "Sampling 12610 points...\n",
      "Sampling 12710 points...\n",
      "Sampling 12810 points...\n",
      "Sampling 12910 points...\n",
      "Sampling 13010 points...\n",
      "Sampling 13110 points...\n",
      "Sampling 13210 points...\n",
      "Sampling 13310 points...\n",
      "Sampling 13410 points...\n",
      "Sampling 13510 points...\n",
      "Sampling 13610 points...\n",
      "Sampling 13710 points...\n",
      "Sampling 13810 points...\n",
      "Sampling 13910 points...\n",
      "Sampling 14010 points...\n",
      "Sampling 14110 points...\n",
      "Sampling 14210 points...\n",
      "Sampling 14310 points...\n",
      "Sampling 14410 points...\n",
      "Sampling 14510 points...\n",
      "Sampling 14610 points...\n",
      "Sampling 14710 points...\n",
      "Sampling 14810 points...\n",
      "Sampling 14910 points...\n",
      "Sampling 15010 points...\n",
      "Sampling 15110 points...\n",
      "Sampling 15210 points...\n",
      "Sampling 15310 points...\n",
      "Sampling 15410 points...\n",
      "Sampling 15510 points...\n",
      "Sampling 15610 points...\n",
      "Sampling 15710 points...\n",
      "Sampling 15810 points...\n",
      "Sampling 15910 points...\n",
      "Sampling 16010 points...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "MODELED_BASIS_FUNCTIONS = 100\n",
    "\n",
    "M = mnist_cnn_embedding\n",
    "\n",
    "c_true = np.linalg.lstsq(M, mnist_y.numpy(), rcond=None)[0]\n",
    "\n",
    "M_TM = M[:, :MODELED_BASIS_FUNCTIONS]\n",
    "\n",
    "N = 20000  # number of samples to select\n",
    "\n",
    "\n",
    "\n",
    "x_axis = range(10, N, 100)\n",
    "errors_random_avg = []\n",
    "errors_leverage_avg = []\n",
    "parameter_errors_random_avg = []\n",
    "parameter_errors_leverage_avg = []\n",
    "\n",
    "\n",
    "for n in x_axis:\n",
    "    errors_random = []\n",
    "    errors_leverage = []\n",
    "    parameter_errors_random = []\n",
    "    parameter_errors_leverage = []\n",
    "\n",
    "    print(f\"Sampling {n} points...\")\n",
    "    for trial in range(10):\n",
    "        random_indices = np.random.choice(M.shape[0], n, replace=False)\n",
    "        leverage_scores = np.linalg.norm(np.linalg.qr(M_TM, mode='reduced')[0], axis=1) ** 2\n",
    "\n",
    "        M_TM_random = M_TM[random_indices, :]\n",
    "        c_random = np.linalg.lstsq(M_TM_random, mnist_y.numpy()[random_indices], rcond=None)[0]\n",
    "        y_random = M_TM @ c_random\n",
    "        error_random = np.linalg.norm(mnist_y.numpy() - y_random) / np.linalg.norm(mnist_y.numpy())\n",
    "        parameter_error_random = np.linalg.norm(c_true[:MODELED_BASIS_FUNCTIONS] - c_random) / np.linalg.norm(c_true[:MODELED_BASIS_FUNCTIONS])\n",
    "\n",
    "        leverage_indices = np.argsort(-leverage_scores)[:n]\n",
    "        M_TM_leverage = M_TM[leverage_indices, :]\n",
    "        c_leverage = np.linalg.lstsq(M_TM_leverage, mnist_y.numpy()[leverage_indices], rcond=None)[0]\n",
    "        y_leverage = M_TM @ c_leverage\n",
    "        error_leverage = np.linalg.norm(mnist_y.numpy() - y_leverage) / np.linalg.norm(mnist_y.numpy())\n",
    "        parameter_error_leverage = np.linalg.norm(c_true[:MODELED_BASIS_FUNCTIONS] - c_leverage) / np.linalg.norm(c_true[:MODELED_BASIS_FUNCTIONS])\n",
    "\n",
    "        errors_random.append(error_random)\n",
    "        errors_leverage.append(error_leverage)\n",
    "        parameter_errors_random.append(parameter_error_random)\n",
    "        parameter_errors_leverage.append(parameter_error_leverage)\n",
    "    errors_random_avg.append(np.mean(errors_random))\n",
    "    errors_leverage_avg.append(np.mean(errors_leverage))\n",
    "    parameter_errors_random_avg.append(np.mean(parameter_errors_random))\n",
    "    parameter_errors_leverage_avg.append(np.mean(parameter_errors_leverage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be28d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(x_axis, errors_random, label='Random Sampling')\n",
    "plt.semilogy(x_axis, errors_leverage, label='Leverage Score Sampling')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.ylabel('Relative Error')\n",
    "plt.title('MNIST Active Learning: Random vs Leverage Score Sampling')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(x_axis, parameter_errors_random, label='Random Sampling')\n",
    "plt.semilogy(x_axis, parameter_errors_leverage, label='Leverage Score Sampling')\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Relative Parameter Error')\n",
    "plt.title('MNIST Active Learning Parameter Error: Random vs Leverage Score Sampling')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f2a161",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 images in color with a resolution of 32x32 pixels, divided into 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). This gives us a 60,000 x 32 x 32 x 3 = 60,000 x 3072 data matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
