{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a17c0683",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "Datasets:\n",
    "1. CIFAR-10\n",
    "2. MNIST\n",
    "\n",
    "Models:\n",
    "1. Convolutional Features\n",
    "2. ReLU Features\n",
    "3. Fourier Features\n",
    "\n",
    "Each model transforms the data to a feature matrix $[M_{TM} | M_{TU}]$ where $M_{TM}$ is the data matrix for the training set and $M_{TU}$ are the basis functions that we have not yet modeled. We will compute the best coefficients, $\\tilde{c}$ of basis functions to model the labels on the modeled training set and the best coefficients, $c$, of all basis functions to model the labels on the whole training set. We will then compute the error $c_{err} = \\tilde{c}-c^*$ where $c^*$ is the truncated version of $c$ to match the size of $\\tilde{c}$. We initialize $c$ with the least-squares coefficients learned from the whole training set. Then we compute $\\tilde{c}$ by solving the least-squares problem on the sampled training set. \n",
    "\n",
    "For each dataset, we will:\n",
    "- Sample the features uniformly at random vs by leverage scores.\n",
    "- Plot $||A||_2$, $||M_{TM}^+||_2$, and $||\\tilde{c}-c^*||_2$ for the sampled features as a function of the number of sampled points.\n",
    "\n",
    "We expect to see that leverage score sampling leads to a smaller error $||\\tilde{c}-c^*||_2$ for the same number of sampled points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29ab21d",
   "metadata": {},
   "source": [
    "\n",
    "## MNIST\n",
    "\n",
    "The MNIST dataset consists of 70,000 images of handwritten digits (0-9) in grayscale with a resolution of 28x28 pixels. This gives us a $70,000 \\times 784$ data matrix.\n",
    "- A Convolutional Neural network will transform the data to a $70,000 \\times 200$ matrix (by removing the last layer).\n",
    "- A Random ReLU fully-connected network ($y({\\textbf{t}}) = \\sum_{k=1}^{200} w_k \\sigma(\\left<\\textbf{t}, {\\textbf{v}}_k\\right>)$ with $\\sigma(x) = \\max(0,x)$ and $\\textbf{v}_k$ being randomly initialized weights and $w_k$ being the learned coefficients) will transform the data to a $70,000 \\times 200$ matrix.\n",
    "- A Fourier fully-connected network ($y({\\textbf{t}}) = \\mathscr{R}(\\sum_{k=1}^{200} w_k \\exp(i\\pi\\left<\\textbf{t}, {\\textbf{v}}_k\\right>)) = \\sum_{k=1}^{200} w_k \\cos(\\pi\\left<\\textbf{t}, {\\textbf{v}}_k\\right>) = $ with $\\textbf{v}_k$ being randomly initialized weights and $w_k$ being the learned coefficients) will transform the data to a $70,000 \\times 200$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de55619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import graphlearning as gl\n",
    "\n",
    "mnist_digits, mnist_labels = gl.datasets.load(\"mnist\")\n",
    "\n",
    "mnist_X = torch.tensor(mnist_digits, dtype=torch.float32).reshape(-1, 1, 28, 28) / 255.0\n",
    "mnist_y = torch.tensor(mnist_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Fourier Features\n",
    "def rff_features(X, features=200) -> torch.Tensor:\n",
    "    N, *_ = X.shape\n",
    "    X = X.reshape(N, -1)\n",
    "\n",
    "    W = torch.randn(X.shape[1], features)\n",
    "\n",
    "    return torch.cos(torch.pi * X @ W) / np.sqrt(features)  # Normalize\n",
    "\n",
    "\n",
    "mnist_rff_features = rff_features(mnist_X, features=2000)\n",
    "\n",
    "\n",
    "# Random ReLU Features\n",
    "def relu_features(X, features=200) -> torch.Tensor:\n",
    "    N, *_ = X.shape\n",
    "    X = X.reshape(N, -1)\n",
    "    W = torch.randn(X.shape[1], features)\n",
    "    return torch.relu(X @ W) / np.sqrt(features)\n",
    "\n",
    "\n",
    "mnist_relu_features = relu_features(mnist_X, features=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49d7db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9846\n"
     ]
    }
   ],
   "source": [
    "# Verify CNN accuracy on MNIST\n",
    "\n",
    "from models.mnist_cnn import ConvNet, BASIS_FUNCTIONS\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "network = ConvNet()\n",
    "network.load_state_dict(torch.load(\"models/mnist_cnn.pth\", map_location=device))\n",
    "network.eval()\n",
    "\n",
    "\n",
    "def verify_mnist_cnn(model: ConvNet, device):\n",
    "    model.to(device)\n",
    "    indices = torch.randperm(mnist_X.shape[0])\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        TensorDataset(mnist_X[indices], mnist_y[indices]),\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds: torch.Tensor = model(xb)\n",
    "            correct += (preds.argmax(dim=1) == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    print(f\"Test accuracy: {correct / total:.4f}\")\n",
    "\n",
    "\n",
    "verify_mnist_cnn(network, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea998e6",
   "metadata": {},
   "source": [
    "We get an accuracy on the whole dataset of `0.9846`. Pretty good. Now we can embed the data using the convolutional layers of the network and use that as our feature matrix for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f8441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70000, 200])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embed the data using the convolutional layers of the network\n",
    "mnist_cnn_embedding = torch.tensor(np.zeros((mnist_X.shape[0], BASIS_FUNCTIONS)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_start in range(0, mnist_X.shape[0], 256):\n",
    "        batch_end = min(batch_start + 256, mnist_X.shape[0])\n",
    "        batch = mnist_X[batch_start:batch_end].to(device)\n",
    "        embeddings = network.embed(batch)\n",
    "        mnist_cnn_embedding[batch_start:batch_end] = embeddings\n",
    "\n",
    "mnist_cnn_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99d91274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def parameter_error(\n",
    "    data: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    c_true: torch.Tensor,\n",
    "    indices: torch.Tensor,\n",
    "):\n",
    "    c_computed = torch.linalg.lstsq(data[indices, :], labels[indices, :]).solution\n",
    "    return (c_true - c_computed).norm() / c_true.norm()\n",
    "\n",
    "\n",
    "def aliasing_op_norm(data: torch.Tensor, indices: torch.Tensor):\n",
    "    return torch.linalg.norm(\n",
    "        torch.linalg.pinv(data[indices, :]) @ data[indices, :], ord=2\n",
    "    )\n",
    "\n",
    "\n",
    "def norm_M_pinv(data: torch.Tensor, indices: torch.Tensor):\n",
    "    return torch.linalg.norm(torch.linalg.pinv(data[indices, :]), ord=2)\n",
    "\n",
    "\n",
    "def simulation(\n",
    "    M: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    sample_points: int = 5000,\n",
    "    sample_point_step: int = 100,\n",
    "    trials: int = 10,\n",
    "):\n",
    "    device = M.device\n",
    "    c_true = torch.linalg.lstsq(M, labels).solution\n",
    "    x_axis = range(sample_point_step, sample_points, sample_point_step)\n",
    "\n",
    "    data = {\n",
    "        metric: {\n",
    "            method: []\n",
    "            for method in [\"random\", \"leverage\", \"random_leverage\"]\n",
    "        }\n",
    "        for metric in [\"parameter_error\", \"aliasing_error\", \"norm_M_pinv\"]\n",
    "    }\n",
    "\n",
    "    for n in x_axis:\n",
    "        print(f\"  Sampling {n} points...\")\n",
    "        total_p_err_random, total_err_random, total_M_pinv_random = 0.0, 0.0, 0.0\n",
    "        total_p_err_levg, total_err_levg, total_M_pinv_levg = 0.0, 0.0, 0.0\n",
    "        total_p_err_rand_levg, total_err_rand_levg, total_M_pinv_rand_levg = (0.0,) * 3\n",
    "\n",
    "        for _ in range(trials):\n",
    "            leverage_scores = (\n",
    "                torch.linalg.norm(torch.linalg.qr(M, mode=\"reduced\").Q, dim=1) ** 2\n",
    "            )\n",
    "\n",
    "            random_indices = torch.randperm(M.shape[0], device=device)[:n]\n",
    "            top_leverage_indices = torch.topk(leverage_scores, n, largest=True).indices\n",
    "            random_leverage_indices = torch.multinomial(\n",
    "                leverage_scores, n, replacement=True\n",
    "            )\n",
    "\n",
    "            for indices, (p_err_storage, aliasing_err_storage, M_pinv_norm_storage) in [\n",
    "                (\n",
    "                    random_indices,\n",
    "                    (total_p_err_random, total_err_random, total_M_pinv_random),\n",
    "                ),\n",
    "                (\n",
    "                    top_leverage_indices,\n",
    "                    (total_p_err_levg, total_err_levg, total_M_pinv_levg),\n",
    "                ),\n",
    "                (\n",
    "                    random_leverage_indices,\n",
    "                    (\n",
    "                        total_p_err_rand_levg,\n",
    "                        total_err_rand_levg,\n",
    "                        total_M_pinv_rand_levg,\n",
    "                    ),\n",
    "                ),\n",
    "            ]:\n",
    "                p_err_storage += parameter_error(M, labels, c_true, indices)\n",
    "                aliasing_err_storage += aliasing_op_norm(M, indices)\n",
    "                M_pinv_norm_storage += norm_M_pinv(M, indices)\n",
    "        \n",
    "        data[\"parameter_error\"][\"random\"].append(total_p_err_random / trials)\n",
    "        data[\"aliasing_error\"][\"random\"].append(total_err_random / trials)\n",
    "        data[\"norm_M_pinv\"][\"random\"].append(total_M_pinv_random / trials)\n",
    "        data[\"parameter_error\"][\"leverage\"].append(total_p_err_levg / trials)\n",
    "        data[\"aliasing_error\"][\"leverage\"].append(total_err_levg / trials)\n",
    "        data[\"norm_M_pinv\"][\"leverage\"].append(total_M_pinv_levg / trials)\n",
    "        data[\"parameter_error\"][\"random_leverage\"].append(\n",
    "            total_p_err_rand_levg / trials\n",
    "        )\n",
    "        data[\"aliasing_error\"][\"random_leverage\"].append(total_err_rand_levg / trials)\n",
    "        data[\"norm_M_pinv\"][\"random_leverage\"].append(total_M_pinv_rand_levg / trials)\n",
    "\n",
    "    return x_axis, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d86a6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating CNN features...\n",
      "-----------------------\n",
      "  Sampling 100 points...\n",
      "  Sampling 200 points...\n",
      "  Sampling 300 points...\n",
      "  Sampling 400 points...\n",
      "  Sampling 500 points...\n",
      "  Sampling 600 points...\n",
      "  Sampling 700 points...\n",
      "  Sampling 800 points...\n",
      "  Sampling 900 points...\n",
      "  Sampling 1000 points...\n",
      "  Sampling 1100 points...\n",
      "  Sampling 1200 points...\n",
      "  Sampling 1300 points...\n",
      "  Sampling 1400 points...\n",
      "  Sampling 1500 points...\n",
      "  Sampling 1600 points...\n",
      "  Sampling 1700 points...\n",
      "  Sampling 1800 points...\n",
      "  Sampling 1900 points...\n",
      "  Sampling 2000 points...\n",
      "  Sampling 2100 points...\n",
      "  Sampling 2200 points...\n",
      "  Sampling 2300 points...\n",
      "  Sampling 2400 points...\n",
      "  Sampling 2500 points...\n",
      "  Sampling 2600 points...\n",
      "  Sampling 2700 points...\n",
      "  Sampling 2800 points...\n",
      "  Sampling 2900 points...\n",
      "  Sampling 3000 points...\n",
      "  Sampling 3100 points...\n",
      "  Sampling 3200 points...\n",
      "  Sampling 3300 points...\n",
      "  Sampling 3400 points...\n",
      "  Sampling 3500 points...\n",
      "  Sampling 3600 points...\n",
      "  Sampling 3700 points...\n",
      "  Sampling 3800 points...\n",
      "  Sampling 3900 points...\n",
      "  Sampling 4000 points...\n",
      "  Sampling 4100 points...\n",
      "  Sampling 4200 points...\n",
      "  Sampling 4300 points...\n",
      "  Sampling 4400 points...\n",
      "  Sampling 4500 points...\n",
      "  Sampling 4600 points...\n",
      "  Sampling 4700 points...\n",
      "  Sampling 4800 points...\n",
      "  Sampling 4900 points...\n",
      "  Sampling 5000 points...\n",
      "  Sampling 5100 points...\n",
      "  Sampling 5200 points...\n",
      "  Sampling 5300 points...\n",
      "  Sampling 5400 points...\n",
      "  Sampling 5500 points...\n",
      "  Sampling 5600 points...\n",
      "  Sampling 5700 points...\n",
      "  Sampling 5800 points...\n",
      "  Sampling 5900 points...\n",
      "  Sampling 6000 points...\n",
      "  Sampling 6100 points...\n",
      "  Sampling 6200 points...\n",
      "  Sampling 6300 points...\n",
      "  Sampling 6400 points...\n",
      "  Sampling 6500 points...\n",
      "  Sampling 6600 points...\n",
      "  Sampling 6700 points...\n",
      "  Sampling 6800 points...\n",
      "  Sampling 6900 points...\n",
      "  Sampling 7000 points...\n",
      "  Sampling 7100 points...\n",
      "  Sampling 7200 points...\n",
      "  Sampling 7300 points...\n",
      "  Sampling 7400 points...\n",
      "  Sampling 7500 points...\n",
      "  Sampling 7600 points...\n",
      "  Sampling 7700 points...\n",
      "  Sampling 7800 points...\n",
      "  Sampling 7900 points...\n",
      "  Sampling 8000 points...\n",
      "  Sampling 8100 points...\n",
      "  Sampling 8200 points...\n",
      "  Sampling 8300 points...\n",
      "  Sampling 8400 points...\n",
      "  Sampling 8500 points...\n",
      "  Sampling 8600 points...\n",
      "  Sampling 8700 points...\n",
      "  Sampling 8800 points...\n",
      "  Sampling 8900 points...\n",
      "  Sampling 9000 points...\n",
      "  Sampling 9100 points...\n",
      "  Sampling 9200 points...\n",
      "  Sampling 9300 points...\n",
      "  Sampling 9400 points...\n",
      "  Sampling 9500 points...\n",
      "  Sampling 9600 points...\n",
      "  Sampling 9700 points...\n",
      "  Sampling 9800 points...\n",
      "  Sampling 9900 points...\n",
      "Simulating RFF features...\n",
      "-----------------------\n",
      "  Sampling 100 points...\n",
      "  Sampling 200 points...\n",
      "  Sampling 300 points...\n",
      "  Sampling 400 points...\n",
      "  Sampling 500 points...\n",
      "  Sampling 600 points...\n",
      "  Sampling 700 points...\n",
      "  Sampling 800 points...\n",
      "  Sampling 900 points...\n",
      "  Sampling 1000 points...\n",
      "  Sampling 1100 points...\n",
      "  Sampling 1200 points...\n",
      "  Sampling 1300 points...\n",
      "  Sampling 1400 points...\n",
      "  Sampling 1500 points...\n",
      "  Sampling 1600 points...\n",
      "  Sampling 1700 points...\n",
      "  Sampling 1800 points...\n",
      "  Sampling 1900 points...\n",
      "  Sampling 2000 points...\n",
      "  Sampling 2100 points...\n",
      "  Sampling 2200 points...\n",
      "  Sampling 2300 points...\n",
      "  Sampling 2400 points...\n",
      "  Sampling 2500 points...\n",
      "  Sampling 2600 points...\n",
      "  Sampling 2700 points...\n",
      "  Sampling 2800 points...\n",
      "  Sampling 2900 points...\n",
      "  Sampling 3000 points...\n",
      "  Sampling 3100 points...\n",
      "  Sampling 3200 points...\n",
      "  Sampling 3300 points...\n",
      "  Sampling 3400 points...\n",
      "  Sampling 3500 points...\n",
      "  Sampling 3600 points...\n",
      "  Sampling 3700 points...\n",
      "  Sampling 3800 points...\n",
      "  Sampling 3900 points...\n",
      "  Sampling 4000 points...\n",
      "  Sampling 4100 points...\n",
      "  Sampling 4200 points...\n",
      "  Sampling 4300 points...\n",
      "  Sampling 4400 points...\n",
      "  Sampling 4500 points...\n",
      "  Sampling 4600 points...\n",
      "  Sampling 4700 points...\n",
      "  Sampling 4800 points...\n",
      "  Sampling 4900 points...\n"
     ]
    }
   ],
   "source": [
    "labels = torch.nn.functional.one_hot(\n",
    "    mnist_y,\n",
    "    num_classes=10,\n",
    ").to(device, dtype=torch.float32)\n",
    "\n",
    "print(\"Simulating CNN features...\")\n",
    "print(\"-----------------------\")\n",
    "cnn_x_axis, cnn_data = simulation(\n",
    "    mnist_cnn_embedding.float(),\n",
    "    labels,\n",
    "    sample_points=10000,\n",
    "    sample_point_step=100,\n",
    "    trials=5,\n",
    ")\n",
    "print(\"Simulating RFF features...\")\n",
    "print(\"-----------------------\")\n",
    "rff_x_axis, rff_data = simulation(\n",
    "    mnist_rff_features.float(),\n",
    "    labels,\n",
    "    sample_points=10000,\n",
    "    sample_point_step=100,\n",
    "    trials=5,\n",
    ")\n",
    "print(\"Simulating ReLU features...\")\n",
    "print(\"-----------------------\")\n",
    "relu_x_axis, relu_data = simulation(\n",
    "    mnist_relu_features.float(),\n",
    "    labels,\n",
    "    sample_points=10000,\n",
    "    sample_point_step=100,\n",
    "    trials=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abedfd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results in 4 by 3 grid (4 metrics, 3 datasets)\n",
    "metrics = [\"parameter_error\", \"aliasing_error\", \"norm_M_pinv\"]\n",
    "extra_metrics = [\"Leverage Scores Distribution\"]\n",
    "datasets = {\n",
    "    \"CNN Features\": (cnn_x_axis, cnn_data),\n",
    "    \"RFF Features\": (rff_x_axis, rff_data),\n",
    "    \"ReLU Features\": (relu_x_axis, relu_data),\n",
    "}\n",
    "methods = [\"random\", \"leverage\", \"random_leverage\"]\n",
    "method_labels = {\n",
    "    \"random\": \"Random Sampling\",\n",
    "    \"leverage\": \"Leverage Score Sampling\",\n",
    "    \"random_leverage\": \"Randomized Leverage Score Sampling\",\n",
    "}\n",
    "metric_labels = {\n",
    "    \"parameter_error\": \"Parameter Error\",\n",
    "    \"aliasing_error\": \"Aliasing Operator Norm\",\n",
    "    \"norm_M_pinv\": \"Norm of Pseudoinverse of $M_{TM}$\",\n",
    "}\n",
    "method_styles = {\n",
    "    \"random\": \"o--\",\n",
    "    \"leverage\": \"s-.\",\n",
    "    \"random_leverage\": \"^-\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(len(metrics) + len(extra_metrics), len(datasets), figsize=(15, 12))\n",
    "\n",
    "for col, (dataset_name, (x_axis, data)) in enumerate(datasets.items()):\n",
    "    for row, metric in enumerate(metrics):\n",
    "        ax = axes[row, col]\n",
    "        for method in methods:\n",
    "            ax.plot(\n",
    "                x_axis,\n",
    "                data[metric][method],\n",
    "                method_styles[method],\n",
    "                label=method_labels[method],\n",
    "            )\n",
    "        ax.set_title(f\"{dataset_name} - {metric.replace('_', ' ').title()}\")\n",
    "        ax.set_xlabel(\"Number of Sampled Points\")\n",
    "        ax.set_ylabel(metric_labels[metric])\n",
    "        ax.grid(True)\n",
    "        if row == 0 and col == 0:\n",
    "            ax.legend()\n",
    "\n",
    "    # Plot leverage scores distribution\n",
    "    ax = axes[len(metrics), col]\n",
    "    if dataset_name == \"CNN Features\":\n",
    "        M = mnist_cnn_embedding.float()\n",
    "    elif dataset_name == \"RFF Features\":\n",
    "        M = mnist_rff_features.float()\n",
    "    else:  # ReLU Features\n",
    "        M = mnist_relu_features.float()\n",
    "\n",
    "    leverage_scores = (\n",
    "        torch.linalg.norm(torch.linalg.qr(M, mode=\"reduced\").Q, dim=1) ** 2\n",
    "    ).cpu().numpy()\n",
    "    ax.hist(leverage_scores, bins=50, density=True)\n",
    "    ax.set_title(f\"{dataset_name} - Leverage Scores Distribution\")\n",
    "    ax.set_xlabel(\"Leverage Score\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"active_learning_mnist.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot leverage scores distribution to observe difference from uniform sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f2a161",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 images in color with a resolution of 32x32 pixels, divided into 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). This gives us a 60,000 x 32 x 32 x 3 = 60,000 x 3072 data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92378599",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 4",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m cifar, cifar_labels = gl.datasets.load(\u001b[33m\"\u001b[39m\u001b[33mcifar10\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m cifar_X = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcifar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m / \u001b[32m255.0\u001b[39m\n\u001b[32m      4\u001b[39m cifar_y = torch.tensor(cifar_labels, dtype=torch.long)\n",
      "\u001b[31mRuntimeError\u001b[39m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 4"
     ]
    }
   ],
   "source": [
    "cifar, cifar_labels = gl.datasets.load(\"cifar10\")\n",
    "\n",
    "cifar_X = torch.tensor(cifar, dtype=torch.float32) / 255.0\n",
    "cifar_y = torch.tensor(cifar_labels, dtype=torch.long)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
