{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a17c0683",
   "metadata": {
    "id": "a17c0683"
   },
   "source": [
    "# Goal\n",
    "\n",
    "Datasets:\n",
    "1. CIFAR-10\n",
    "2. MNIST\n",
    "\n",
    "Models:\n",
    "1. Convolutional Features\n",
    "2. ReLU Features\n",
    "3. Fourier Features\n",
    "\n",
    "Each model transforms the data to a feature matrix $[M_{TM} | M_{TU}]$ where $M_{TM}$ is the data matrix for the training set and $M_{TU}$ are the basis functions that we have not yet modeled. We will compute the best coefficients, $\\tilde{c}$ of basis functions to model the labels on the modeled training set and the best coefficients, $c$, of all basis functions to model the labels on the whole training set. We will then compute the error $c_{err} = \\tilde{c}-c^*$ where $c^*$ is the truncated version of $c$ to match the size of $\\tilde{c}$. We initialize $c$ with the least-squares coefficients learned from the whole training set. Then we compute $\\tilde{c}$ by solving the least-squares problem on the sampled training set.\n",
    "\n",
    "## Bernoulli Sampling with Leverage Scores\n",
    "\n",
    "We treat the leverage scores as a bernoulli sampling probability for each data point, and sample accordingly. \n",
    "\n",
    "We then calculate the error between the least-squares solution found on the sampled matrix versus the full matrix.\n",
    "\n",
    "We then plot each sample a plot of $(n, \\text{error})$.\n",
    "\n",
    "For each dataset, we will:\n",
    "- Sample the features uniformly at random vs by leverage scores.\n",
    "- Plot $||A||_2$, $||M_{TM}^+||_2$, and $||\\tilde{c}-c^*||_2$ for the sampled features as a function of the number of sampled points.\n",
    "\n",
    "We expect to see that leverage score sampling leads to a smaller error $||\\tilde{c}-c^*||_2$ for the same number of sampled points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66885cd",
   "metadata": {},
   "source": [
    "\n",
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "874f8441",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "874f8441",
    "outputId": "b19fca77-47a5-4356-f99b-7a2a52e161e6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "BASIS_FUNCTIONS = 200\n",
    "\n",
    "# Embed the testing set (not the training set)\n",
    "def embed_dataset(X, model, device):\n",
    "    # Embed the data using the convolutional layers of the network\n",
    "    embeddings = torch.tensor(\n",
    "        np.zeros(\n",
    "            (\n",
    "                X.shape[0],\n",
    "                BASIS_FUNCTIONS,\n",
    "            )\n",
    "        )\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_start in range(0, X.shape[0], 256):\n",
    "            batch_end = min(batch_start + 256, X.shape[0])\n",
    "            batch = X[batch_start:batch_end].to(device)\n",
    "            batch_embeddings = model.embed(batch)\n",
    "            embeddings[batch_start:batch_end] = batch_embeddings\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Random Fourier Features\n",
    "def rff_features(X, features=200) -> torch.Tensor:\n",
    "    N, *_ = X.shape\n",
    "    X = X.reshape(N, -1)\n",
    "\n",
    "    W = torch.randn(X.shape[1], features, device=X.device)\n",
    "\n",
    "    return torch.cos(torch.pi * X @ W) / np.sqrt(features)  # Normalize\n",
    "\n",
    "\n",
    "# Random ReLU Features\n",
    "def relu_features(X, features=200) -> torch.Tensor:\n",
    "    N, *_ = X.shape\n",
    "    X = X.reshape(N, -1)\n",
    "    W = torch.randn(X.shape[1], features, device=X.device)\n",
    "    return torch.relu(X @ W) / np.sqrt(features)\n",
    "\n",
    "\n",
    "def leverage_scores(A: torch.Tensor) -> torch.Tensor:\n",
    "    q, _ = torch.linalg.qr(A, mode=\"reduced\")\n",
    "    return torch.sum(q**2, dim=1)\n",
    "\n",
    "\n",
    "def sample_bernoulli(leverage_scores: torch.Tensor) -> torch.Tensor:\n",
    "    random_values = torch.rand((leverage_scores.shape[0],), device=leverage_scores.device)\n",
    "    sampled_indices = random_values < leverage_scores\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "def least_squares_solution(A: torch.Tensor, b: torch.Tensor, regularizer: float = 1e-12, weights: torch.Tensor | None = None) -> torch.Tensor:\n",
    "    A = A.float()\n",
    "    b = b.float()\n",
    "    if weights is not None:\n",
    "        A *= torch.sqrt(weights)[:, None]\n",
    "        b *= torch.sqrt(weights)[:, None]\n",
    "    return torch.linalg.lstsq(\n",
    "        A.T @ A + regularizer * torch.eye(A.shape[1], device=A.device),\n",
    "        A.T @ b,\n",
    "    ).solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29ab21d",
   "metadata": {
    "id": "c29ab21d"
   },
   "source": [
    "\n",
    "## MNIST\n",
    "\n",
    "The MNIST dataset consists of 70,000 images of handwritten digits (0-9) in grayscale with a resolution of 28x28 pixels. This gives us a $70,000 \\times 784$ data matrix.\n",
    "- A Convolutional Neural network will transform the data to a $70,000 \\times 200$ matrix (by removing the last layer).\n",
    "- A Random ReLU fully-connected network ($y({\\textbf{t}}) = \\sum_{k=1}^{200} w_k \\sigma(\\left<\\textbf{t}, {\\textbf{v}}_k\\right>)$ with $\\sigma(x) = \\max(0,x)$ and $\\textbf{v}_k$ being randomly initialized weights and $w_k$ being the learned coefficients) will transform the data to a $70,000 \\times 200$ matrix.\n",
    "- A Fourier fully-connected network ($y({\\textbf{t}}) = \\mathscr{R}(\\sum_{k=1}^{200} w_k \\exp(i\\pi\\left<\\textbf{t}, {\\textbf{v}}_k\\right>)) = \\sum_{k=1}^{200} w_k \\cos(\\pi\\left<\\textbf{t}, {\\textbf{v}}_k\\right>) = $ with $\\textbf{v}_k$ being randomly initialized weights and $w_k$ being the learned coefficients) will transform the data to a $70,000 \\times 200$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0de55619",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0de55619",
    "outputId": "af635215-8a78-44c8-b8c2-19618545b3b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using\", DEVICE)\n",
    "\n",
    "mnist_X = (\n",
    "    MNIST(root=\"./data\", train=True, download=True)\n",
    "    .data.float()\n",
    "    .to(DEVICE)\n",
    "    .reshape(-1, 1, 28, 28)\n",
    "    / 255.0\n",
    ")\n",
    "mnist_y = MNIST(root=\"./data\", train=True, download=True).targets.to(DEVICE)\n",
    "test_mnist_X = (\n",
    "    MNIST(root=\"./data\", train=False, download=True)\n",
    "    .data.float()\n",
    "    .to(DEVICE)\n",
    "    .reshape(-1, 1, 28, 28)\n",
    "    / 255.0\n",
    ")\n",
    "test_mnist_y = MNIST(root=\"./data\", train=False, download=True).targets.to(DEVICE)\n",
    "\n",
    "combined_mnist_X = torch.cat([mnist_X, test_mnist_X], dim=0)\n",
    "combined_mnist_y = torch.cat([mnist_y, test_mnist_y], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e49d7db0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e49d7db0",
    "outputId": "e5173c95-5617-4d3d-8cfa-5e9d75919ea2"
   },
   "outputs": [],
   "source": [
    "# Verify CNN accuracy on MNIST\n",
    "\n",
    "from models.mnist_cnn import MnistConvNet, BASIS_FUNCTIONS\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "mnist_network = MnistConvNet()\n",
    "mnist_network.load_state_dict(torch.load(\"models/mnist_cnn.pth\", map_location=DEVICE))\n",
    "mnist_network.eval()\n",
    "\n",
    "\n",
    "def verify_mnist_cnn(model: MnistConvNet, device):\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        TensorDataset(test_mnist_X, test_mnist_y),\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds: torch.Tensor = model(xb)\n",
    "            correct += (preds.argmax(dim=1) == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    print(f\"Test accuracy: {correct / total:.4f}\")\n",
    "\n",
    "\n",
    "# verify_mnist_cnn(network, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e40a649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist cnn on cpu\n",
      "torch.Size([70000, 200])\n",
      "mnist rff on cpu\n",
      "torch.Size([70000, 200])\n",
      "mnist relu on cpu\n",
      "torch.Size([70000, 200])\n"
     ]
    }
   ],
   "source": [
    "mnist_cnn_embedding = embed_dataset(combined_mnist_X, model=mnist_network, device=DEVICE)\n",
    "print(\"mnist cnn on\", mnist_cnn_embedding.device)\n",
    "print(mnist_cnn_embedding.shape)\n",
    "\n",
    "mnist_rff_features = rff_features(combined_mnist_X, features=200)\n",
    "print(\"mnist rff on\", mnist_rff_features.device)\n",
    "print(mnist_rff_features.shape)\n",
    "\n",
    "mnist_relu_features = relu_features(combined_mnist_X, features=200)\n",
    "print(\"mnist relu on\", mnist_relu_features.device)\n",
    "print(mnist_relu_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d86a6eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d86a6eb",
    "outputId": "bbd2d564-124c-486d-c556-cd0500bdb67e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing embedding: MNIST CNN\n",
      "  Processing beta: 1.0\n",
      "  Processing beta: 6.2105263157894735\n",
      "  Processing beta: 11.421052631578947\n",
      "  Processing beta: 16.63157894736842\n",
      "  Processing beta: 21.842105263157894\n",
      "  Processing beta: 27.052631578947366\n",
      "  Processing beta: 32.26315789473684\n",
      "  Processing beta: 37.473684210526315\n",
      "  Processing beta: 42.68421052631579\n",
      "  Processing beta: 47.89473684210526\n",
      "  Processing beta: 53.10526315789473\n",
      "  Processing beta: 58.315789473684205\n",
      "  Processing beta: 63.526315789473685\n",
      "  Processing beta: 68.73684210526315\n",
      "  Processing beta: 73.94736842105263\n",
      "  Processing beta: 79.1578947368421\n",
      "  Processing beta: 84.36842105263158\n",
      "  Processing beta: 89.57894736842105\n",
      "  Processing beta: 94.78947368421052\n",
      "  Processing beta: 100.0\n",
      "Processing embedding: MNIST RFF\n",
      "  Processing beta: 1.0\n",
      "  Processing beta: 6.2105263157894735\n",
      "  Processing beta: 11.421052631578947\n",
      "  Processing beta: 16.63157894736842\n",
      "  Processing beta: 21.842105263157894\n",
      "  Processing beta: 27.052631578947366\n",
      "  Processing beta: 32.26315789473684\n",
      "  Processing beta: 37.473684210526315\n",
      "  Processing beta: 42.68421052631579\n",
      "  Processing beta: 47.89473684210526\n",
      "  Processing beta: 53.10526315789473\n",
      "  Processing beta: 58.315789473684205\n",
      "  Processing beta: 63.526315789473685\n",
      "  Processing beta: 68.73684210526315\n",
      "  Processing beta: 73.94736842105263\n",
      "  Processing beta: 79.1578947368421\n",
      "  Processing beta: 84.36842105263158\n",
      "  Processing beta: 89.57894736842105\n",
      "  Processing beta: 94.78947368421052\n",
      "  Processing beta: 100.0\n",
      "Processing embedding: MNIST ReLU\n",
      "  Processing beta: 1.0\n",
      "  Processing beta: 6.2105263157894735\n",
      "  Processing beta: 11.421052631578947\n",
      "  Processing beta: 16.63157894736842\n",
      "  Processing beta: 21.842105263157894\n",
      "  Processing beta: 27.052631578947366\n",
      "  Processing beta: 32.26315789473684\n",
      "  Processing beta: 37.473684210526315\n",
      "  Processing beta: 42.68421052631579\n",
      "  Processing beta: 47.89473684210526\n",
      "  Processing beta: 53.10526315789473\n",
      "  Processing beta: 58.315789473684205\n",
      "  Processing beta: 63.526315789473685\n",
      "  Processing beta: 68.73684210526315\n",
      "  Processing beta: 73.94736842105263\n",
      "  Processing beta: 79.1578947368421\n",
      "  Processing beta: 84.36842105263158\n",
      "  Processing beta: 89.57894736842105\n",
      "  Processing beta: 94.78947368421052\n",
      "  Processing beta: 100.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "labels = torch.nn.functional.one_hot(\n",
    "    combined_mnist_y,\n",
    "    num_classes=10,\n",
    ").to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "\n",
    "TRIALS = 100\n",
    "# BETAS = 10 ** np.linspace(0, np.log10(50_000), 20)\n",
    "BETAS = np.linspace(1, 100, 20)\n",
    "EMBEDDINGS = {\n",
    "    \"MNIST CNN\": mnist_cnn_embedding,\n",
    "    \"MNIST RFF\": mnist_rff_features,\n",
    "    \"MNIST ReLU\": mnist_relu_features,\n",
    "}\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(len(EMBEDDINGS), 2, figsize=(10 * len(EMBEDDINGS), 10))\n",
    "\n",
    "for i, (embedding_name, embedding) in enumerate(EMBEDDINGS.items()):\n",
    "    print(\"Processing embedding:\", embedding_name)\n",
    "    c_full = least_squares_solution(embedding, labels)\n",
    "\n",
    "    leverage = leverage_scores(embedding)\n",
    "\n",
    "    # Plot leverage scores\n",
    "    leverage_cpu = leverage.cpu().numpy()\n",
    "    axs[i, 1].plot(np.arange(1, len(leverage_cpu) + 1), np.sort(leverage_cpu)[::-1], marker=\"o\", linestyle=\"\")\n",
    "    axs[i, 1].set_xlabel(\"Leverage Score\")\n",
    "    axs[i, 1].set_ylabel(\"Frequency\")\n",
    "    axs[i, 1].set_title(f\"Leverage Score Distribution for {embedding_name}\")\n",
    "    axs[i, 1].set_xscale(\"log\")\n",
    "\n",
    "    for j, beta in enumerate(BETAS):\n",
    "        print(\"  Processing beta:\", beta)\n",
    "        errors = []\n",
    "        sample_sizes = []\n",
    "        for trial in range(TRIALS):\n",
    "            sampled_indices = sample_bernoulli(torch.clamp(leverage * beta, max=1.0))\n",
    "            A_sampled = embedding[sampled_indices, :]\n",
    "            b_sampled = labels[sampled_indices, :]\n",
    "            c_sampled = least_squares_solution(A_sampled, b_sampled, weights=1/leverage[sampled_indices])\n",
    "            sample_size = A_sampled.shape[0]\n",
    "            error = torch.linalg.matrix_norm(c_sampled - c_full).item()\n",
    "            errors.append(error)\n",
    "            sample_sizes.append(sample_size)\n",
    "        axs[i, 0].plot(sample_sizes, errors, \"o\", color=plt.cm.viridis(j / len(BETAS)))\n",
    "    axs[i, 0].set_xlabel(\"Sample Size\")\n",
    "    axs[i, 0].set_ylabel(\"Error Norm\")\n",
    "    axs[i, 0].set_title(f\"Embedding {embedding_name}\")\n",
    "    axs[i, 0].set_yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"bernoulli_sampling_errors_mnist.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f2a161",
   "metadata": {
    "id": "46f2a161"
   },
   "source": [
    "## CIFAR-10\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 images in color with a resolution of 32x32 pixels, divided into 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). This gives us a 60,000 x 32 x 32 x 3 = 60,000 x 3072 data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92378599",
   "metadata": {
    "id": "92378599"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from models.cifar_cnn import (\n",
    "    BiggestCifarConvNet,\n",
    "    CifarConvNet,\n",
    "    BiggerCifarConvNet,\n",
    ")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cifar_images = (\n",
    "    torch.tensor(CIFAR10(root=\"./data\", train=True, download=True).data)\n",
    "    .to(DEVICE)\n",
    "    .permute(0, 3, 1, 2)\n",
    "    .float()\n",
    "    / 255.0\n",
    ")\n",
    "cifar_labels = torch.tensor(\n",
    "    CIFAR10(root=\"./data\", train=True, download=True).targets, device=DEVICE\n",
    ")\n",
    "test_cifar_images = (\n",
    "    torch.tensor(CIFAR10(root=\"./data\", train=False, download=True).data)\n",
    "    .to(DEVICE)\n",
    "    .permute(0, 3, 1, 2)\n",
    "    .float()\n",
    "    / 255.0\n",
    ")\n",
    "test_cifar_labels = torch.tensor(\n",
    "    CIFAR10(root=\"./data\", train=False, download=True).targets, device=DEVICE\n",
    ")\n",
    "combined_cifar_images = torch.cat([cifar_images, test_cifar_images], dim=0)\n",
    "combined_cifar_labels = torch.cat([cifar_labels, test_cifar_labels], dim=0)\n",
    "\n",
    "cifar_network = CifarConvNet()\n",
    "cifar_network.load_state_dict(torch.load(\"models/cifar_cnn.pth\", map_location=DEVICE))\n",
    "cifar_network.eval()\n",
    "\n",
    "bigger_cifar_network = BiggerCifarConvNet()\n",
    "bigger_cifar_network.load_state_dict(\n",
    "    torch.load(\"models/cifar_bigger_cnn.pth\", map_location=DEVICE)\n",
    ")\n",
    "bigger_cifar_network.eval()\n",
    "\n",
    "biggest_cifar_network = BiggestCifarConvNet()\n",
    "biggest_cifar_network.load_state_dict(\n",
    "    torch.load(\"models/cifar_biggest_cnn.pth\", map_location=DEVICE)\n",
    ")\n",
    "biggest_cifar_network.eval()\n",
    "\n",
    "\n",
    "def verify_cifar_cnn(model, device):\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(test_cifar_images, test_cifar_labels),\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds: torch.Tensor = model(xb)\n",
    "            correct += (preds.argmax(dim=1) == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    print(f\"  Test accuracy: {correct / total:.4f}\")\n",
    "\n",
    "\n",
    "# print(\"Verifying CIFAR-10 CNN model accuracy:\")\n",
    "# verify_cifar_cnn(cifar_network, DEVICE)\n",
    "# print(\"Verifying Bigger CIFAR-10 CNN model accuracy:\")\n",
    "# verify_cifar_cnn(bigger_network, DEVICE)\n",
    "# print(\"Verifying Biggest CIFAR-10 CNN model accuracy:\")\n",
    "# verify_cifar_cnn(biggest_network, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99da0ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar cnn on cpu\n",
      "torch.Size([60000, 200])\n",
      "cifar bigger cnn on cpu\n",
      "torch.Size([60000, 200])\n",
      "cifar biggest cnn on cpu\n",
      "torch.Size([60000, 200])\n",
      "cifar rff on cpu\n",
      "torch.Size([60000, 200])\n",
      "cifar relu on cpu\n",
      "torch.Size([60000, 200])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cifar_cnn_embedding = embed_dataset(combined_cifar_images, model=cifar_network, device=DEVICE)\n",
    "print(\"cifar cnn on\", cifar_cnn_embedding.device)\n",
    "print(cifar_cnn_embedding.shape)\n",
    "\n",
    "cifar_bigger_cnn_embedding = embed_dataset(\n",
    "    combined_cifar_images, model=bigger_cifar_network, device=DEVICE\n",
    ")\n",
    "print(\"cifar bigger cnn on\", cifar_bigger_cnn_embedding.device)\n",
    "print(cifar_bigger_cnn_embedding.shape)\n",
    "\n",
    "cifar_biggest_cnn_embedding = embed_dataset(\n",
    "    combined_cifar_images, model=biggest_cifar_network, device=DEVICE\n",
    ")\n",
    "print(\"cifar biggest cnn on\", cifar_biggest_cnn_embedding.device)\n",
    "print(cifar_biggest_cnn_embedding.shape)\n",
    "\n",
    "cifar_rff_features = rff_features(combined_cifar_images, features=200)\n",
    "print(\"cifar rff on\", cifar_rff_features.device)\n",
    "print(cifar_rff_features.shape)\n",
    "\n",
    "cifar_relu_features = relu_features(combined_cifar_images, features=200)\n",
    "print(\"cifar relu on\", cifar_relu_features.device)\n",
    "print(cifar_relu_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1e7b1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing embedding: CIFAR CNN\n",
      "  Processing beta: 1.0\n",
      "  Processing beta: 6.2105263157894735\n",
      "  Processing beta: 11.421052631578947\n",
      "  Processing beta: 16.63157894736842\n",
      "  Processing beta: 21.842105263157894\n",
      "  Processing beta: 27.052631578947366\n",
      "  Processing beta: 32.26315789473684\n",
      "  Processing beta: 37.473684210526315\n",
      "  Processing beta: 42.68421052631579\n",
      "  Processing beta: 47.89473684210526\n",
      "  Processing beta: 53.10526315789473\n",
      "  Processing beta: 58.315789473684205\n",
      "  Processing beta: 63.526315789473685\n",
      "  Processing beta: 68.73684210526315\n",
      "  Processing beta: 73.94736842105263\n",
      "  Processing beta: 79.1578947368421\n",
      "  Processing beta: 84.36842105263158\n",
      "  Processing beta: 89.57894736842105\n",
      "  Processing beta: 94.78947368421052\n",
      "  Processing beta: 100.0\n",
      "Processing embedding: CIFAR Bigger CNN\n",
      "  Processing beta: 1.0\n",
      "  Processing beta: 6.2105263157894735\n",
      "  Processing beta: 11.421052631578947\n",
      "  Processing beta: 16.63157894736842\n",
      "  Processing beta: 21.842105263157894\n",
      "  Processing beta: 27.052631578947366\n",
      "  Processing beta: 32.26315789473684\n",
      "  Processing beta: 37.473684210526315\n",
      "  Processing beta: 42.68421052631579\n",
      "  Processing beta: 47.89473684210526\n",
      "  Processing beta: 53.10526315789473\n",
      "  Processing beta: 58.315789473684205\n",
      "  Processing beta: 63.526315789473685\n",
      "  Processing beta: 68.73684210526315\n",
      "  Processing beta: 73.94736842105263\n",
      "  Processing beta: 79.1578947368421\n",
      "  Processing beta: 84.36842105263158\n",
      "  Processing beta: 89.57894736842105\n",
      "  Processing beta: 94.78947368421052\n",
      "  Processing beta: 100.0\n",
      "Processing embedding: CIFAR Biggest CNN\n",
      "  Processing beta: 1.0\n",
      "  Processing beta: 6.2105263157894735\n",
      "  Processing beta: 11.421052631578947\n",
      "  Processing beta: 16.63157894736842\n",
      "  Processing beta: 21.842105263157894\n",
      "  Processing beta: 27.052631578947366\n",
      "  Processing beta: 32.26315789473684\n",
      "  Processing beta: 37.473684210526315\n",
      "  Processing beta: 42.68421052631579\n",
      "  Processing beta: 47.89473684210526\n",
      "  Processing beta: 53.10526315789473\n",
      "  Processing beta: 58.315789473684205\n",
      "  Processing beta: 63.526315789473685\n",
      "  Processing beta: 68.73684210526315\n",
      "  Processing beta: 73.94736842105263\n",
      "  Processing beta: 79.1578947368421\n",
      "  Processing beta: 84.36842105263158\n",
      "  Processing beta: 89.57894736842105\n",
      "  Processing beta: 94.78947368421052\n",
      "  Processing beta: 100.0\n",
      "Processing embedding: CIFAR RFF\n",
      "  Processing beta: 1.0\n",
      "  Processing beta: 6.2105263157894735\n",
      "  Processing beta: 11.421052631578947\n",
      "  Processing beta: 16.63157894736842\n",
      "  Processing beta: 21.842105263157894\n",
      "  Processing beta: 27.052631578947366\n",
      "  Processing beta: 32.26315789473684\n",
      "  Processing beta: 37.473684210526315\n",
      "  Processing beta: 42.68421052631579\n",
      "  Processing beta: 47.89473684210526\n",
      "  Processing beta: 53.10526315789473\n",
      "  Processing beta: 58.315789473684205\n",
      "  Processing beta: 63.526315789473685\n",
      "  Processing beta: 68.73684210526315\n",
      "  Processing beta: 73.94736842105263\n",
      "  Processing beta: 79.1578947368421\n",
      "  Processing beta: 84.36842105263158\n",
      "  Processing beta: 89.57894736842105\n",
      "  Processing beta: 94.78947368421052\n",
      "  Processing beta: 100.0\n",
      "Processing embedding: CIFAR ReLU\n",
      "  Processing beta: 1.0\n",
      "  Processing beta: 6.2105263157894735\n",
      "  Processing beta: 11.421052631578947\n",
      "  Processing beta: 16.63157894736842\n",
      "  Processing beta: 21.842105263157894\n",
      "  Processing beta: 27.052631578947366\n",
      "  Processing beta: 32.26315789473684\n",
      "  Processing beta: 37.473684210526315\n",
      "  Processing beta: 42.68421052631579\n",
      "  Processing beta: 47.89473684210526\n",
      "  Processing beta: 53.10526315789473\n",
      "  Processing beta: 58.315789473684205\n",
      "  Processing beta: 63.526315789473685\n",
      "  Processing beta: 68.73684210526315\n",
      "  Processing beta: 73.94736842105263\n",
      "  Processing beta: 79.1578947368421\n",
      "  Processing beta: 84.36842105263158\n",
      "  Processing beta: 89.57894736842105\n",
      "  Processing beta: 94.78947368421052\n",
      "  Processing beta: 100.0\n"
     ]
    }
   ],
   "source": [
    "labels = torch.nn.functional.one_hot(\n",
    "    combined_cifar_labels,\n",
    "    num_classes=10,\n",
    ").to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "\n",
    "TRIALS = 100\n",
    "BETAS = np.linspace(1, 100, 20)\n",
    "EMBEDDINGS = {\n",
    "    \"CIFAR CNN\": cifar_cnn_embedding,\n",
    "    \"CIFAR Bigger CNN\": cifar_bigger_cnn_embedding,\n",
    "    \"CIFAR Biggest CNN\": cifar_biggest_cnn_embedding,\n",
    "    \"CIFAR RFF\": cifar_rff_features,\n",
    "    \"CIFAR ReLU\": cifar_relu_features,\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(len(EMBEDDINGS), 2, figsize=(10 * len(EMBEDDINGS), 10))\n",
    "\n",
    "for i, (embedding_name, embedding) in enumerate(EMBEDDINGS.items()):\n",
    "    print(\"Processing embedding:\", embedding_name)\n",
    "    c_full = least_squares_solution(embedding, labels)\n",
    "\n",
    "    leverage = leverage_scores(embedding)\n",
    "\n",
    "    # Plot leverage scores\n",
    "    leverage_cpu = leverage.cpu().numpy()\n",
    "    axs[i, 1].plot(np.arange(1, len(leverage_cpu) + 1), np.sort(leverage_cpu)[::-1], marker=\"o\", linestyle=\"none\")\n",
    "    axs[i, 1].set_xlabel(\"Leverage Score\")\n",
    "    axs[i, 1].set_ylabel(\"Frequency\")\n",
    "    axs[i, 1].set_title(f\"Leverage Score Distribution for {embedding_name}\")\n",
    "    axs[i, 1].set_xscale(\"log\")\n",
    "\n",
    "    for j, beta in enumerate(BETAS):\n",
    "        print(\"  Processing beta:\", beta)\n",
    "        errors = []\n",
    "        sample_sizes = []\n",
    "        for trial in range(TRIALS):\n",
    "            sampled_indices = sample_bernoulli(torch.clamp(leverage * beta, max=1.0))\n",
    "            A_sampled = embedding[sampled_indices, :]\n",
    "            b_sampled = labels[sampled_indices, :]\n",
    "            c_sampled = least_squares_solution(A_sampled, b_sampled, weights=1/leverage[sampled_indices])\n",
    "            sample_size = A_sampled.shape[0]\n",
    "            error = torch.linalg.matrix_norm(c_sampled - c_full).item()\n",
    "            errors.append(error)\n",
    "            sample_sizes.append(sample_size)\n",
    "        axs[i, 0].plot(sample_sizes, errors, \"o\", color=plt.cm.viridis(j / len(BETAS)))\n",
    "    axs[i, 0].set_xlabel(\"Sample Size\")\n",
    "    axs[i, 0].set_ylabel(\"Error Norm\")\n",
    "    axs[i, 0].set_title(f\"Embedding {embedding_name}\")\n",
    "    axs[i, 0].set_yscale(\"log\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"bernoulli_sampling_errors_cifar.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
